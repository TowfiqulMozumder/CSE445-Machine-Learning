# -*- coding: utf-8 -*-
"""Car Price Prediction- Machine Learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/188bkftZwX0DsZqKWuF_sXzeE7xJp4nFT
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)

c = pd.read_csv("/content/drive/My Drive/CSE445/Car Price Dataset.csv")

c.head()

c.describe()

c['symboling'].astype('category').value_counts()

c['aspiration'].astype('category').value_counts()

c['drivewheel'].astype('category').value_counts()

sns.distplot(c['wheelbase'],color='navy')

sns.distplot(c['curbweight'],color='navy')

sns.distplot(c['stroke'],color='navy')
plt.show()

sns.distplot(c['compressionratio'],color='navy')
plt.show()

fig, ax= plt.subplots(1,2,figsize=(15,5))

sns.distplot(c['price'], norm_hist=False, kde=False, ax=ax[0], color='blue')
ax[0].set_xlabel('Price of Car')
ax[0].set_ylabel('Count of cars',size=12)
ax[0].set_title('Count Of Cars By Price',size=15,weight="bold")

sns.distplot(c['price'], kde=True, ax=ax[1], color='green')
ax[1].set_xlabel('Price of Car')
ax[1].set_ylabel('Cars Relative Frequency',size=12)
ax[1].set_title('Cars Relative Frequency By Price',size=15,weight="bold")

c_n=c.select_dtypes(include=['float64','int64'])
c_n.head()

c_numeric = c_n.drop(['symboling', 'car_ID'], axis=1)
c_numeric.head()

plt.figure(figsize=(8, 8))
sns.pairplot(c_numeric)

for i, col in enumerate (c_numeric.columns):
    plt.figure(i,figsize=(12,8))
    sns.scatterplot(x=c_numeric[col],y=c_numeric['price'],color="navy")

correalation=c_numeric.corr()

plt.figure(figsize=(12,8))
sns.heatmap(correalation,annot=True,cmap="BuPu")

c.info()

c['symboling'] = c['symboling'].astype('object')

c_names = c['CarName'].apply(lambda x: x.split(" ")[0])
c_names[:21]

c['c_comp']=c_names
c['c_comp'].value_counts()

c.loc[(c['c_comp']=="vw")|(c['c_comp']=="vokswagen"),"c_comp"]="volkswagen"
c.loc[(c['c_comp']=="porcshce"),"c_comp"]="porsche"
c.loc[(c['c_comp']=="toyouta"),"c_comp"]="toyota"
c.loc[c['c_comp'] == "Nissan", 'c_comp'] = 'nissan'
c.loc[c['c_comp'] == "maxda", 'c_comp'] = 'mazda'

c['c_comp'].value_counts()

plt.rcParams['figure.figsize'] = [15,8]
ax=c['c_comp'].value_counts().plot(kind='bar',stacked=True, colormap = 'Set1')
ax.title.set_text('Car Company')
plt.xlabel("Names of the Car",fontweight = 'bold')
plt.ylabel("Count of Cars",fontweight = 'bold')

plt.figure(figsize=(20, 15))
plt.subplot(3,3,1)
sns.boxplot(x = 'doornumber', y = 'price', data = c)
plt.subplot(3,3,2)
sns.boxplot(x = 'fueltype', y = 'price', data =  c)
plt.subplot(3,3,3)
sns.boxplot(x = 'aspiration', y = 'price', data =  c)
plt.subplot(3,3,4)
sns.boxplot(x = 'carbody', y = 'price', data = c)
plt.subplot(3,3,5)
sns.boxplot(x = 'enginelocation', y = 'price', data =  c)
plt.subplot(3,3,6)
sns.boxplot(x = 'drivewheel', y = 'price', data = c)
plt.subplot(3,3,7)
sns.boxplot(x = 'enginetype', y = 'price', data =  c)
plt.subplot(3,3,8)
sns.boxplot(x = 'cylindernumber', y = 'price', data = c)
plt.subplot(3,3,9)
sns.boxplot(x = 'fuelsystem', y = 'price', data =  c)
plt.show()

c = c.drop('CarName', axis=1)

x=c.drop(columns=['price',"car_ID"])
y=c['price']

c_categ = x.select_dtypes(include=['object'])
c_categ.head()

c_dummy_var = pd.get_dummies(c_categ, drop_first=True)
c_dummy_var.head()

x=x.drop(columns=c_categ)
x.head(5)

c_dummy_var.columns

df_x=pd.merge(x,c_dummy_var,on=x.index)
df_x.columns

df_x.drop(columns='key_0',inplace=True)
df_x.info()

from sklearn.preprocessing import scale
cols=df_x.columns
df_x_scaled=pd.DataFrame(scale(df_x))
df_x_scaled.columns=cols
df_x_scaled.columns
df_x_scaled.describe()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(df_x_scaled, y, 
                                                    train_size=0.7,
                                                    test_size = 0.3, random_state=100)

from sklearn import linear_model
from sklearn.linear_model import LinearRegression
lm=LinearRegression()
lm.fit(x_train,y_train)

y_pred_test=lm.predict(x_test)
y_pred_train=lm.predict(x_train)

from sklearn.metrics import r2_score

print('R-sqaure on train data: {}'.format(r2_score(y_true=y_train, y_pred=y_pred_train)))
print('R-sqaure on test data: {}'.format(r2_score(y_true=y_test, y_pred=y_pred_test)))

#Standard error/RMSE
error_train=y_pred_train-y_train
error_test=y_pred_test-y_test

print('RMSE on train data: {}'.format(((error_train**2).mean())**0.5))
print('RMSE on test data: {}'.format(((error_test**2).mean())**0.5))



from sklearn.feature_selection import RFE
import statsmodels.api as sm

n_features_list = list(range(4, 31))
train_adjusted_r2 = []
train_r2 = []
test_r2 = []
train_RMSE=[]
test_RMSE=[]

for n_features in range(4, 31):
    lm = LinearRegression()
    rfe_n = RFE(estimator=lm, n_features_to_select=n_features)
    rfe_n.fit(x_train, y_train)
    col_n = x_train.columns[rfe_n.support_]
    X_train_rfe_n = x_train[col_n]
    X_test_rfe_n = x_test[col_n]

    X_train_rfe_n = sm.add_constant(X_train_rfe_n)


    X_test_rfe_n = sm.add_constant(X_test_rfe_n, has_constant='add')

    
    lm_n = sm.OLS(y_train, X_train_rfe_n).fit()
    
    y_pred_test = lm_n.predict(X_test_rfe_n)
    y_pred_train = lm_n.predict(X_train_rfe_n)
    
    
    train_adjusted_r2.append(lm_n.rsquared_adj)
    train_r2.append(lm_n.rsquared)
    test_r2.append(r2_score(y_test, y_pred_test))
    
    error_test=y_pred_test-y_test
    error_train=y_pred_train-y_train
    
    test_RMSE.append(((error_test**2).mean())**0.5)
    train_RMSE.append(((error_train**2).mean())**0.5)

import matplotlib.ticker as plticker

fig,ax=plt.subplots(2,1,figsize=(10, 9))
ax[0].plot(n_features_list, train_r2,'r', label="R-squared_train data")
ax[0].plot(n_features_list, test_r2,'b', label="R-squared_test data")
ax[0].set_xlabel('Features Count')

ax[0].legend(loc='upper left')
loc = plticker.MultipleLocator(base=1)
ax[0].xaxis.set_major_locator(loc) 

ax[1].plot(n_features_list, train_RMSE, 'r',label="RMSE_train data")
ax[1].plot(n_features_list, test_RMSE, 'b',label="RMSE_test data")
ax[1].set_xlabel('Features Count')


ax[1].legend(loc='upper left')
plt.xticks(np.arange(0, 31, step=1))

plt.show()



lm=LinearRegression()
rfe=RFE(lm,13)
rfe.fit(x_train,y_train)

col=x_train.columns[rfe.support_] 

X_train_13= x_train[col]
X_test_13 = x_test[col]

X_train_13 = sm.add_constant(X_train_13,has_constant='add')
X_test_13 = sm.add_constant(X_test_13,has_constant='add')

lm_sm=sm.OLS(y_train,X_train_13).fit()

y_pred_train=lm_sm.predict(X_train_13)
y_pred_test=lm_sm.predict(X_test_13)

train_r2=lm_sm.rsquared
test_r2=r2_score(y_pred_test, y_test)


error_test=y_pred_test-y_test
error_train=y_pred_train-y_train
    
test_RMSE=(((error_test**2).mean())**0.5)
train_RMSE=(((error_train**2).mean())**0.5)

print('R-sqaure:')
print("R-sq for test data is {}".format(test_r2))
print("R-sq for train data is {}".format(train_r2))


print('RMSE: ')
print("RMSE for test data is {}".format(test_RMSE))
print("RMSE for train data is {}".format(train_RMSE))

print(lm_sm.summary())

plt.style.use('ggplot')
fig, ax=plt.subplots(figsize=(13,6))
sns.lineplot(x=y_test.index,y=y_test,label='Actual Price',color='Green',ax=ax)
sns.lineplot(x=y_test.index,y=y_pred_test,label='Predicted Price',color='red',ax=ax)
ax.set_title('Price: Actuals vs Predictions')
ax.set_ylabel('Price')
ax.set_xlabel('Index')

features_13=lm_sm.params.index
features_13=features_13[1:] 
features_13

def linearity_test(model, y):
    
    fitted_vals = model.predict()
    residuals = model.resid
    
    sns.set_style('darkgrid')
    fig,ax=plt.subplots(1,2, figsize=(15,4))
    
    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'}, color='blue')
    ax[0].set_title('Observed vs. Predicted Values', fontsize=16)
    ax[0].set_xlabel('Predicted', fontsize=13)
    ax[0].set_ylabel('Observed', fontsize=13)
    sns.regplot(x=fitted_vals,y=residuals,lowess=True,ax=ax[1],line_kws={'color': 'red'},color='Green')
    ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)
    ax[1].set_xlabel('Predicted', fontsize=13)
    ax[1].set_ylabel('Residuals', fontsize=13)
    
    
linearity_test(lm_sm, y_train)

import statsmodels.stats.api as sms

def homoscedasticity_test(model):
    
    fitted_vals = model.predict()
    residuals = model.resid
    resids_standardized = model.get_influence().resid_studentized_internal
    
    sns.set_style('darkgrid')
    
    
    fig, ax = plt.subplots(1,2,figsize=(15,4))

    sns.regplot(x=fitted_vals, y=residuals, lowess=True, ax=ax[0], line_kws={'color': 'red'},color='green')
    ax[0].set_title('Residuals vs Fitted', fontsize=16)
    ax[0].set_xlabel('Predicted', fontsize=13)
    ax[0].set_ylabel('Residuals', fontsize=13)

    sns.regplot(x=fitted_vals, y=np.sqrt(np.abs(resids_standardized)), lowess=True, ax=ax[1], line_kws={'color': 'red'},color='blue')
    ax[1].set_title('Scale-Location', fontsize=16)
    ax[1].set_xlabel('Predicted', fontsize=13)
    ax[1].set_ylabel('Square Root(Standardized Residuals)', fontsize=13)

      
homoscedasticity_test(lm_sm)

from scipy import stats

def normality_of_residuals_test(model):
    '''
    Function for drawing the normal QQ-plot of the residuals and running 4 statistical tests to 
    investigate the normality of residuals.
    
    Arg:
    * model - fitted OLS models from statsmodels
    '''
    
    sm.ProbPlot(model.resid).qqplot(line='s');
    plt.title('Q-Q Plot');
    

    jb = stats.jarque_bera(model.resid)
    sw = stats.shapiro(model.resid)
    ad = stats.anderson(model.resid, dist='norm')
    ks = stats.kstest(model.resid, 'norm')
    
    print(f'Jarque_Bera test ---- statistic: {jb[0]:.4f}, p-value: {jb[1]}')
    print(f'Shapiro_Wilk test ---- statistic: {sw[0]:.4f}, p-value: {sw[1]:.4f}')
    print(f'Kolmogorov_Smirnov test ---- statistic: {ks.statistic:.4f}, p-value: {ks.pvalue:.4f}')
    print(f'Anderson_Darling test ---- statistic: {ad.statistic:.4f}, 5% critical value: {ad.critical_values[2]:.4f}')
   
normality_of_residuals_test(lm_sm)

def influential_outlier_test(model,top_influencing_obs_count):
    
    influence = model.get_influence()

    leverage = influence.hat_matrix_diag

    cooks_d = influence.cooks_distance

    standardized_residuals = influence.resid_studentized_internal

    studentized_residuals = influence.resid_studentized_external 
    
    
    plot_lm = plt.figure(figsize=(15,5))
    plt.scatter(leverage, standardized_residuals, alpha=0.5,color='blue')
    sns.regplot(leverage, standardized_residuals,scatter=False,ci=False,lowess=True,
                line_kws={'color': 'blue', 'lw': 1, 'alpha': 0.8})
    plot_lm.axes[0].set_xlim(0, max(leverage)+0.01)
    plot_lm.axes[0].set_ylim(-10, 6)
    plot_lm.axes[0].set_title('Standardized Residuals vs Leverage',fontsize=16)
    plot_lm.axes[0].set_xlabel('Leverage',fontsize=13)
    plot_lm.axes[0].set_ylabel('Standardized Residuals',fontsize=13);
    
    leverage_top_n_obs = np.flip(np.argsort(cooks_d[0]), 0)[:top_influencing_obs_count]  
    
    for i in leverage_top_n_obs:
        plot_lm.axes[0].annotate(i,xy=(leverage[i],studentized_residuals[i])) 
    
    def graph(formula, x_range, label=None):
        x = x_range
        y = formula(x)
        plt.plot(x, y, label=label, lw=1, ls='--', color='Blue')

    p = len(lm_sm.params) 

    graph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x), np.linspace(0.001, max(leverage), 50),'Cook\'s distance')
    plt.legend(loc='upper right');
    
influential_outlier_test(model=lm_sm,top_influencing_obs_count=10)

X_train_no_outliers=x_train.drop(index=[16,24,91])
y_train_no_outliers=y_train.drop(index=[16,24,91])
X_train_no_outliers.head()

from sklearn.feature_selection import RFE
import statsmodels.api as sm

n_features_list = list(range(4, 31)) 
train_adjusted_r2 = []
train_r2 = []
test_r2 = []
train_RMSE=[]
test_RMSE=[]

for n_features in range(4, 31):

    lm = LinearRegression()

    rfe_n = RFE(estimator=lm, n_features_to_select=n_features)  

    rfe_n.fit(X_train_no_outliers, y_train_no_outliers)

    col_n = X_train_no_outliers.columns[rfe_n.support_] 

    X_train_rfe_n = X_train_no_outliers[col_n]
    X_test_rfe_n = x_test[col_n]

    X_train_rfe_n = sm.add_constant(X_train_rfe_n,has_constant='add')


    X_test_rfe_n = sm.add_constant(X_test_rfe_n, has_constant='add')

    lm_n = sm.OLS(y_train_no_outliers, X_train_rfe_n).fit()
    
    y_pred_test = lm_n.predict(X_test_rfe_n)
    y_pred_train = lm_n.predict(X_train_rfe_n)
    
    train_adjusted_r2.append(lm_n.rsquared_adj)
    train_r2.append(lm_n.rsquared)
    test_r2.append(r2_score(y_test, y_pred_test))
    
    error_test=y_pred_test-y_test
    error_train=y_pred_train-y_train_no_outliers
    
    test_RMSE.append(((error_test**2).mean())**0.5)
    train_RMSE.append(((error_train**2).mean())**0.5)

fig,ax=plt.subplots(2,1,figsize=(13, 9))
plt.subplots_adjust(hspace = 0.3)
ax[0].plot(n_features_list, train_r2,'b', label="R-squared_train data",color= 'Red')
ax[0].plot(n_features_list, test_r2,'g', label="R-squared_test data", color='purple')
ax[0].set_xlabel('Features Count',fontsize=13)
ax[0].set_ylabel('R-Squared',fontsize=13)
ax[0].set_title('R-Squared (R2) by number of features in the model',fontsize=16)


ax[1].plot(n_features_list, train_RMSE, 'b',label="Root Mean Square Error(RMSE)_train data",color='red')
ax[1].plot(n_features_list, test_RMSE, 'g',label="Root Mean Square Error (RMSE)_test data",color='purple')
ax[1].set_xlabel('Features Count',fontsize=13)
ax[1].set_ylabel('RMSE',fontsize=13)
ax[1].set_title('Root Mean Square Error by number of features in the model',fontsize=16)

ax[0].legend(loc='upper left')
ax[1].legend(loc='upper right')
plt.xticks(np.arange(0,31,1))

plt.show()

RMSE_test_dividedby_train = [i / j for i, j in zip(test_RMSE, train_RMSE)]
RMSE_test_dividedby_train

X_new_cv = df_x_scaled.drop(index=[16,24,91])
y_new_cv = y.drop(index=[16,24,91])

X_new_cv.reset_index(inplace=True, drop=True)
X_new_cv.head(20)

y_new_cv.reset_index(drop=True,inplace=True)
y_new_cv.head(20)

print(X_new_cv.shape,y_new_cv.shape)

from sklearn.model_selection import KFold


K=5 #using 5 folds
kf = KFold(n_splits=K, shuffle=True, random_state=42)
   
for n_features in range(5,31):
    
    train_RMSE = []
    test_RMSE = []
    train_r2=[]
    test_r2=[]
    
    for train, test in kf.split(X_new_cv):
        
        lm = LinearRegression()
        
        rfe_n = RFE(estimator=lm, n_features_to_select=n_features)
        
        rfe_n.fit(X_new_cv.loc[train],y_new_cv[train])
        
        y_pred_train=rfe_n.predict(X_new_cv.loc[train])
        y_pred_test=rfe_n.predict(X_new_cv.loc[test])
        
        #R-square
        train_r2.append(r2_score(y_pred_train , y_new_cv[train]))
        test_r2.append(r2_score(y_pred_test , y_new_cv[test]))
        
        #Error
        error_train = y_pred_train - y_new_cv[train]
        error_test = y_pred_test - y_new_cv[test]
        rmse_train=((error_train**2).mean())**0.5
        rmse_test=((error_test**2).mean())**0.5
        
        train_RMSE.append(rmse_train)
        test_RMSE.append(rmse_test)
        
    test_times_train=np.mean(test_RMSE)/np.mean(train_RMSE)
         # generate report
    print('n-features:{:1} |train_R-squared:{:2} |test_R-squared:{:3} |mean(RMSE(Standard Error)_train):{:4} |mean(RMSE(Standard Error)_test):{:5} |Root Mean Square Error(test/train):{}'.
          format(n_features, round(np.mean(train_r2),4), round(np.mean(test_r2),4),
                 round(np.mean(train_RMSE),0),
                 round(np.mean(test_RMSE),0),round(test_times_train,2)))

import statsmodels.api as sm

lm = LinearRegression()

rfe = RFE(estimator=lm, n_features_to_select=6)

rfe.fit(X_new_cv, y_new_cv)

col= X_new_cv.columns[rfe.support_] 

X_final=X_new_cv[col] #X DF wuith top 6 features only

X_final= sm.add_constant(X_final,has_constant='add')

lm_sm=sm.OLS(y_new_cv,X_final).fit()

print(lm_sm.summary())

from statsmodels.regression.linear_model import OLS
from statsmodels.tools.tools import add_constant

def variance_inflation_factors(X_df): #X_df = X_train normally, in this model X=X_final
    '''
     '''
    X_df = add_constant(X_df)
    vifs = pd.Series(
        [1 / (1. - OLS(X_df[col].values, 
                       X_df.loc[:, X_df.columns != col].values).fit().rsquared) 
         for col in X_df],
        index=X_df.columns,
        name='VIF'
    )
    return vifs

variance_inflation_factors(X_final)

X_final =X_final.loc[:,X_final.columns !='curbweight']
X_final.head()

lm_sm=sm.OLS(y_new_cv,X_final).fit()

y_predictions=lm_sm.predict(X_final)


#Standard error/RMSE
error=y_predictions-y_new_cv

print('RMSE is: {}'.format(((error**2).mean())**0.5))

print(lm_sm.summary())

fig, ax=plt.subplots(figsize=(16,6))
sns.lineplot(x=y_new_cv.index,y=y_new_cv,label='Actuals',color='Red',ax=ax)
sns.lineplot(x=y_new_cv.index,y=y_predictions,label='Predictions',color='Purple',ax=ax)
ax.set_title('Actual Price vs Predicted Price', fontsize=16)
ax.set_ylabel('Car Price',fontsize=13)

linearity_test(lm_sm,y_new_cv)

homoscedasticity_test(lm_sm)